{
    "abstract": "In this paper, we propose a unified view of gradient-based algorithms forstochastic convex composite optimization by extending the concept of estimatesequence introduced by Nesterov. More precisely, we interpret a large class of stochasticoptimization methods as procedures that iteratively minimize a surrogate ofthe objective, which covers the stochastic gradient descent method and variants of the incremental approaches SAGA, SVRG, and MISO/Finito/SDCA.This point of view has several advantages: (i) we provide a simple genericproof of convergence for all of the aforementioned methods; (ii) we naturallyobtain new algorithms with the same guarantees; (iii) we derive genericstrategies to make these algorithms robust to stochastic noise, which is usefulwhen data is corrupted by small random perturbations. Finally, we propose a newaccelerated stochastic gradient descent algorithm anda new accelerated SVRG algorithm that is robust to stochastic noise.",
    "authors": [
        "Andrei Kulunchakov",
        "Julien Mairal"
    ],
    "emails": [
        "andrei.kulunchakov@inria.fr",
        "julien.mairal@inria.fr"
    ],
    "id": "19-073",
    "issue": 155,
    "pages": [
        1,
        52
    ],
    "title": "Estimate Sequences for Stochastic Composite Optimization: Variance Reduction, Acceleration, and Robustness to Noise",
    "volume": 21,
    "year": 2020
}