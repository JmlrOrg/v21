{
    "abstract": "Stochastic gradient Langevin dynamics (SGLD) is a fundamental algorithm in stochastic optimization. Recent work by Zhang et al. (2017) presents an analysis for the hitting time of SGLD for the first and second order stationary points. The proof in  Zhang et al. (2017) is a two-stage procedure through bounding the Cheeger's constant, which is rather complicated and leads to loose bounds.  In this paper,  using intuitions from stochastic differential equations,  we provide a direct analysis for the hitting times of SGLD to the first and second order stationary points. Our analysis is straightforward. It only relies on basic linear algebra and probability theory tools. Our direct analysis also leads to tighter bounds comparing to Zhang et al. (2017) and shows the explicit dependence of the hitting time on different factors, including dimensionality, smoothness, noise strength, and step size effects.  Under suitable conditions,  we show that the hitting time of SGLD to first-order stationary points can be dimension-independent. Moreover, we apply our analysis to study several important online estimation problems in machine learning, including linear regression, matrix factorization, and online PCA.",
    "authors": [
        "Xi Chen",
        "Simon S. Du",
        "Xin T. Tong"
    ],
    "emails": [
        "xchen3@stern.nyu.edu",
        "ssdu@ias.edu",
        "Mattxin@nus.edu.sg"
    ],
    "id": "19-327",
    "issue": 68,
    "pages": [
        1,
        41
    ],
    "title": "On Stationary-Point Hitting Time and Ergodicity of Stochastic Gradient Langevin Dynamics",
    "volume": 21,
    "year": 2020
}